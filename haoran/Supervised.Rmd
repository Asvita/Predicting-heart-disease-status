---
title: "Supervised"
date: "2019-5-15"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ISLR)
library(factoextra)
library(gridExtra)
library(corrplot)
library(RColorBrewer)
library(gplots)
library(tidyverse)
library(caret)
library(MLmetrics)
library(rpart.plot)
library(randomForest)
library(pdp)
library(glmnet)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_bw() + theme(legend.position = "bottom"))

```

```{r}
heart_disease = read_csv("..\\data\\heart.csv") %>% 
    mutate(target = ifelse(target==1, 0, 1)) %>% 
    mutate(target=as.factor(target)) %>%
    mutate(target=as.factor(ifelse(target==0, "absence", "presence")))

# %>%  mutate(target = relevel(target, "absence")) 
#  %>%   arrange(-as.numeric(target))
set.seed(1)
#trRows = createDataPartition(heart_disease$target, p = .75, list = FALSE)
#train = heart_disease[trRows,]
#test = heart_disease[-trRows,]

# heart_disease2 = read_csv("..\\data\\heart.csv") %>% 
#     mutate(target = ifelse(target==1, 0, 1)) %>% 
#     mutate(target=as.factor(heart_disease$target)) 

heart_disease = heart_disease %>% 
    filter(thal != 0) %>% 
    mutate(sex=as.factor(sex),
           cp=as.factor(cp),
           fbs=as.factor(fbs),
           restecg=as.factor(restecg),
           exang=as.factor(exang),
           slope=as.factor(slope),
           thal=factor(thal))

model.x <- model.matrix(target~.,heart_disease)[,-1]
model.y <- heart_disease$target

# test = test %>% 
#     mutate(sex=as.factor(sex),
#            cp=as.factor(cp),
#            fbs=as.factor(fbs),
#            restecg=as.factor(restecg),
#            exang=as.factor(exang),
#            slope=as.factor(slope),
#            thal=as.factor(thal))
# test.x <- model.matrix(target~.,test)[,-1]
# test.y <- test$target
```

## Regularized logistic
```{r}
ctrl = trainControl(method = "cv",
                    classProbs = TRUE,
                    summaryFunction = twoClassSummary)


glmnGrid <- expand.grid(.alpha = seq(0, 0.5, length = 10),
                        .lambda = exp(seq(-10,-1, length = 100)))
set.seed(1)
model.glm <- train(x = model.x,
                   y = model.y,
                   method = "glmnet",
                   tuneGrid = glmnGrid,
                   metric = "ROC",
                   trControl = ctrl)
```


```{r}
ggplot(model.glm, highlight = T)  +
    viridis::scale_color_viridis(discrete = TRUE) + 
    scale_shape_manual(values = seq(1,10))

model.glm$bestTune
```

```{r}
glmnet = glmnet(x = model.x, y = model.y, 
       family = "binomial", 
       alpha = 0, 
       lambda = 0.1946867)
broom::tidy(glmnet)
```




## LDA
```{r, message=FALSE}
set.seed(1)
model.lda = train(x = model.x,
                   y = model.y,
                  method = "lda",
                  metric = "ROC",
                  trControl = ctrl)
```


## Naive bayes
```{r, warning=FALSE}
set.seed(1)
nbGrid = expand.grid(usekernel = c(FALSE,TRUE),
                     fL = 1, adjust = seq(0, 4, length = 20))
model.bayes = train(x = model.x,
                    y = model.y,
                    method = "nb",
                    tuneGrid = nbGrid,
                    metric = "ROC",
                    trControl = ctrl) 

ggplot(model.bayes, highlight = T)

model.bayes$bestTune
```

##Tree

```{r, message=FALSE}

set.seed(1)
tree.class <- train(model.x, model.y,
                    method = "rpart",
                    tuneGrid = data.frame(cp = exp(seq(-10,-3, len = 20))),
                    trControl = ctrl,
                    metric = "ROC")
ggplot(tree.class, highlight = TRUE)
tree.class$bestTune

rpart.plot(tree.class$finalModel)
```

##Bagging

```{r, message=FALSE}
bagging.grid <- expand.grid(mtry = 18,
                            splitrule = "gini",
                            min.node.size = 1:40)

set.seed(1)
bagging.class <- train(model.x, model.y,
                method = "ranger",
                tuneGrid = bagging.grid,
                metric = "ROC",
                trControl = ctrl,
                importance = "impurity")
ggplot(bagging.class, highlight = TRUE)
bagging.class$bestTune

barplot(sort(ranger::importance(bagging.class$finalModel),
             decreasing = FALSE),
las = 2, horiz = TRUE, cex.names = 0.7,
col = colorRampPalette(colors = c("darkred","white","darkblue"))(18))
```

##Random Forest

```{r, message=FALSE}
rf.grid <- expand.grid(mtry = 1:6,
                       splitrule = "gini",
                       min.node.size = seq(1,191, by = 2))

set.seed(1)
rf.class <- train(model.x, model.y,
                  method = "ranger",
                  tuneGrid = rf.grid,
                  metric = "ROC",
                  trControl = ctrl,
                  importance = "impurity")


ggplot(rf.class, highlight = TRUE) +
    viridis::scale_color_viridis(discrete = TRUE) + 
    scale_shape_manual(values = seq(1,7))

barplot(sort(ranger::importance(rf.class$finalModel), decreasing = FALSE),
las = 2, horiz = TRUE, cex.names = 0.7,
col = colorRampPalette(colors = c("darkred","white","darkblue"))(18))

a = predict(rf.class, type="raw")

```

##Boosting

```{r fig.height=12, fig.width=12}
boost.grid <- expand.grid(n.trees = seq(20, 1700, by = 25),
                          interaction.depth = 2:7,
                          shrinkage =  seq(0.01, 0.06, by = 0.005),
                          n.minobsinnode = 1)

set.seed(1)
# Adaboost loss function
boost.class = train(model.x, model.y,
                    tuneGrid = boost.grid,
                    trControl = ctrl,
                    method = "gbm",
                    distribution = "adaboost",
                    metric = "ROC",
                    verbose = FALSE)

ggplot(boost.class, highlight = TRUE) +
    viridis::scale_color_viridis(discrete = TRUE) + 
    scale_shape_manual(values = seq(0,10))

summary(boost.class$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

###centered ICE

```{r}


ice_ca.rf = rf.class %>%
    pdp::partial(pred.var = "thalach",
            grid.resolution = 100,
            ice = TRUE, 
            prob = TRUE) %>%
    autoplot(train = heart_disease, alpha = .1, center = TRUE) +
    ggtitle("Random forest, thalach")

ice_oldpeak.rf =  rf.class %>%
    partial(pred.var = "oldpeak",
            grid.resolution = 100,
            ice = TRUE,
            prob = TRUE) %>%
    autoplot(train = heart_disease, alpha = .1) +
    ggtitle("Random forest, oldpeak")

grid.arrange(ice_ca.rf, ice_oldpeak.rf, nrow = 1)
```


## SVM
```{r, message=FALSE}

ctrl2 <- trainControl(method = "cv")

## linear boundary
set.seed(1)
svml.fit <- train(target~., 
                  data = heart_disease, 
                  method = "svmLinear2",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cost = exp(seq(-5,0,len=50))),
                  trControl = ctrl2)

ggplot(svml.fit, highlight = TRUE)
svml.fit$bestTune

## radial kernel
svmr.grid <- expand.grid(C = exp(seq(-4,5,len=50)),
                         sigma = exp(seq(-5,-2,len=10)))
set.seed(1)             
svmr.fit <- train(target~., 
                  data = heart_disease,
                  method = "svmRadial",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  trControl = ctrl2)

ggplot(svmr.fit, highlight = TRUE) +
    viridis::scale_color_viridis(discrete = TRUE) + 
    scale_shape_manual(values = seq(1,10))
svmr.fit$bestTune

#resamp <- resamples(list(svmr = svmr.fit, 
#                         svml = svml.fit))
#bwplot(resamp)
#summary(resamp)
```

## Neural network
```{r, message=FALSE}
nnetGrid <- expand.grid(size = seq(from = 22, to = 40, by = 2), 
                        decay = seq(from = 2.5, to = 6.5, length = 41))

set.seed(1)
cnnet.fit <- train(target~.,
                   heart_disease,
                   method = "nnet",
                   tuneGrid = nnetGrid,
                   preProcess = c("center","scale"),
                   trControl = ctrl,
                   metric = "ROC",
                   trace = FALSE)

ggplot(cnnet.fit, highlight = TRUE) + 
    viridis::scale_color_viridis(discrete = TRUE) + 
    scale_shape_manual(values = seq(1,13))
```

```{r}
resamp = resamples(list(
                        glm.fit = model.glm,
                        lda.fit = model.lda,
                        bayes.fit = model.bayes,
                        boost = boost.class, 
                        rf = rf.class, 
                        bagging = bagging.class, 
                        tree = tree.class,
                        cnnet.fit = cnnet.fit
                        ))
summary(resamp)
bwplot(resamp, metric = "ROC")
```

#Comparing accuracy

##Regularized logistic

